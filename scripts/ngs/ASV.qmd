# DADA2 workflow for ASVs

This script used the DADA2 pipeline to analyze NGS data to generate ASVs and optionally cluster them based on pairwise distances or though taxonomic classification. The ASVs can be saved as FASTA or CSV files for further processing with for example the associated python script that assigns taxonomic data to each ASV using either ezbiocloud or SILVA data base.

It is recommended to run this file from the top down to make sure all libraries are loaded but most code blocks should load the libraries needed for that part. However, they need to be installed first, in many places the code for install the libraries is commented out.

If desired run fastqc on the raw data

```{bash}
#| label: Run FASTqc on the raw reads
# Set to false to skip this part
if false; then
  source /Users/kasper/miniconda3/etc/profile.d/conda.sh
  conda activate bioinfo_env
  # Define input/output directories
  SAMPLE_PRO="phe" 
  RAW_DIR="../../data/ngs/raw_reads/phe"
  OUT_DIR="../../data/ngs/fastqc/raw_phe"
  
  # Create output directory if it doesn't exist
  mkdir -p "$OUT_DIR"
  
  # Run FastQC on all fastq.gz files in the input folder
  fastqc -t 4 -o "$OUT_DIR" "$RAW_DIR"/*.fastq.gz
fi
```

Load files modify to fit the correct path

```{r}
#| label: Load Raw reads
#if (!requireNamespace("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
#BiocManager::install("dada2")

library(dada2)
# packageVersion("dada2")
# Define file paths V1
# Name F = sample1_R1_001.fastq.gz, Name R = sample1_R2_001.fastq.gz
data_path <- "../../data/ngs/"
process_sample <- "phe"
path <- paste0(data_path, paste0("raw_reads/", process_sample))
list.files(path)

fnFs <- sort(list.files(path, pattern="R1.*\\.fastq\\.gz$", full.names=TRUE))
fnRs <- sort(list.files(path, pattern="R2.*\\.fastq\\.gz$", full.names=TRUE))
sample.names <- sapply(basename(fnFs), function(x) sub("_R1.*", "", x))
```

Get quality profile, turned off with #\| eval: false but can be used as a simpler form of fastqc

```{r}
#| label: Plot Quality Profiles
#| eval: false
# Plot the quality of the reads
plotQualityProfile(fnFs[1:3])  # a couple of forward samples
plotQualityProfile(fnRs[1:3])  # a couple of reverse samples
```

Filter and Trim reads with DADA2

```{r echo=FALSE}
#| label: Filter and Trim
# Filter reads (adjust parameters)
filt_path <- file.path(data_path, "filtered")
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

todo <- !(file.exists(filtFs) & file.exists(filtRs))  # need both halves
sum(todo)   # how many still to do?

out <- filterAndTrim(
  fnFs[todo], filtFs[todo], # input & filtered forward files
  fnRs[todo], filtRs[todo], # input & filtered reverse files
  truncLen=c(285,225),      # truncate forward reads at x bp, reverse at y bp
  maxN=0,                   # discard reads with any Ns (ambiguous bases)
  maxEE=c(2,2),             # expected errors ≤2 for both reads
  truncQ=2,                 # truncate reads at first base with quality ≤2
  rm.phix=TRUE,             # remove reads matching the phiX genome
  compress=TRUE,            # gzip output
  multithread=TRUE,         # use multiple cores
  #OMP = FALSE,             # disable OpenMP
  verbose = TRUE            # show status
)
head(out)
```

Rerun Fastqc after the filtering if needed

```{bash}
#| label: FASTqc after filtering
if false; then
  source /Users/kasper/miniconda3/etc/profile.d/conda.sh
  conda activate bioinfo_env
  # Define input/output directories
  RAW_DIR="./original-NGS-data/285-filtered"
  OUT_DIR="./post-285-filtered-fastqc-reports"
  
  # Create output directory if it doesn't exist
  mkdir -p "$OUT_DIR"
  
  # Run FastQC on all fastq.gz files in the input folder
  fastqc -t 4 -o "$OUT_DIR" "$RAW_DIR"/*.fastq.gz
fi
```

Learn errors and de-noise

```{r echo=FALSE}
#| label: Learn Errors
# Errors
errFs <- learnErrors(filtFs, multithread = TRUE) # randomize = TRUE?
errRs <- learnErrors(filtRs, multithread = TRUE)
# Visualize
#plotErrors(errFs, nominalQ=TRUE)
#plotErrors(errRs, nominalQ=TRUE)
# Denoise
derepFs <- derepFastq(filtFs)
derepRs <- derepFastq(filtRs)
dadaF <- dada(derepFs, err = errFs, multithread = TRUE)
dadaR <- dada(derepRs, err = errRs, multithread = TRUE)
```

Merge paired ends, construct table and remove chimeras

```{r echo=FALSE}
#| label: Merge pairs and make sequence table
mergers <- mergePairs(
  dadaF, derepFs,
  dadaR, derepRs,
  minOverlap=12,     # require ≥12 bp overlap
  maxMismatch=0,     # no mismatches allowed in the overlap
  verbose=TRUE,
  returnRejects = TRUE
)

seqtab <- makeSequenceTable(mergers)
seqtab.nochim <- removeBimeraDenovo(
  seqtab,
  method="consensus",    # “pooled” or “consensus” approaches
  multithread=TRUE,
  verbose=TRUE
)
```

Convert the table into more easy to read format with separate tables one for the counts and one for the IDs and corresponding sequences

```{r}
#| label: Generate data frames and save results 
#if (!requireNamespace("Biostrings", quietly=TRUE))
#    BiocManager::install("Biostrings")
#     BiocManager::install("DECIPHER")
#     BiocManager::install("phyloseq")
#     install.packages("pwalign")

library(Biostrings) # For DNA sequence handling
library(DECIPHER)    # For sequence alignment and distance calculation
library(phyloseq) 
library(pwalign)
library(tidyr)


# Get the vector of ASV sequences
asv_seqs <- colnames(seqtab.nochim)
asv_seq_lenght <- unlist(lapply(asv_seqs, nchar))

# Make short IDs
asv_ids <- paste0("ASV", seq_along(asv_seqs))

# Build the count table with IDs instead of full sequences
asv_counts <- seqtab.nochim
colnames(asv_counts) <- asv_ids
rownames(asv_counts) <- sub("_.*", "", rownames(asv_counts))

# Build the mapping table
asv_map <- data.frame(
  ASV_ID   = asv_ids,
  Sequence = asv_seqs,
  ASV_Lenght = asv_seq_lenght,
  stringsAsFactors = FALSE
)

# If we want to save the data from this run
folder_path = "../../results/ngs/"

if (!dir.exists(folder_path)) dir.create(folder_path)

write.csv(asv_counts, paste0(folder_path, "ASV_counts_ids.csv"), quote=FALSE)
write.csv(asv_map, paste0(folder_path, "ASV_id_sequence_map.csv"), quote=FALSE, row.names=FALSE)

# Also save the counts in long form with and without the sequences which can easily be loaded and turned into the asv counts with pivot_wider()
asv_raw_df <- as.data.frame(asv_counts)
asv_raw_df$Sample <- rownames(asv_raw_df)
raw_ASV_counts <- pivot_longer(asv_raw_df, 
                         cols = -Sample,
                         names_to = "ASV", 
                         values_to = "Counts")
raw_ASV_counts <- raw_ASV_counts %>%
  left_join(asv_map, by = c("ASV" = "ASV_ID"))

# Save the long forms
write.csv(raw_ASV_counts %>% select(-Sequence), paste0(folder_path, "ASVs-raw-counts-no-seq.csv"))
write.csv(raw_ASV_counts, paste0(folder_path, "ASVs-raw-counts-seq.csv"))

# Also sava fasta file for all ASVs which we can run blast on
dna <- DNAStringSet(asv_map$Sequence) # If all ASVs should be saved just use the asv_seqs as argument

names(dna) <- sprintf("%s",
                      asv_map$ASV_ID)
writeXStringSet(dna,
                filepath = paste("all_asvs", process_sample,".fasta"), #All-ASV-variants
                format   = "fasta")
```
